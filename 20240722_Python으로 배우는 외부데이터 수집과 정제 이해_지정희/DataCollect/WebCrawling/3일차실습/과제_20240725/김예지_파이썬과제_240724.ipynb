{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kQYKpjbWadf-"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dA1GQfcadgA"},"outputs":[],"source":["# class 안에 특수문자 있는 경우 \\\\ 처리 해야함\n","# 제목, 링크, 날짜, 카테고리, 답변수\n","\n","data=[]\n","pages = int(input('찾는 페이지 수 입력 : '))\n","for i in range(1,pages+1):\n","\n","    url = f'https://kin.naver.com/search/list.naver?query=%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0&page={i}'\n","    response = requests.get(url)\n","\n","    if response.status_code==200:\n","        html = response.text\n","        soup = BeautifulSoup(html,'html.parser')\n","        # print(soup.prettify())\n","        items = soup.select('.basic1>li')\n","        # print(items)\n","\n","\n","        for naverin in items:\n","            title = naverin.select_one('dt').text.strip()\n","            # print(title)\n","            link = naverin.select_one('.basic1>li>dl>dt>a').attrs['href'].strip()\n","            # print(link)\n","            date = naverin.select_one('.txt_inline').text.strip()\n","            # print(date)\n","            categori = naverin.select_one('.txt_block').contents[3].text.strip()\n","            # print(categori)\n","            answer = naverin.select_one('.hit').text.strip()\n","            # print(answer)\n","        #     # print(title,link,date,categori,answer)\n","            data.append([title,link,date,categori,answer])\n","        df = pd.DataFrame(data,columns = ['제목', '링크', '날짜', '카테고리', '답변수'])\n","        # print(df)\n","        df.to_excel('naverin.xlsx',index=False)\n","        if soup.select_one('.next,_nclicks\\\\:kin\\\\.next')== None: # 페이지가 없으면 멈춤\n","            break\n","\n","    else:\n","        print(\"실패\",response.status_code)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5--nGC2VadgA"},"outputs":[],"source":["# class 안에 특수문자 있는 경우 \\\\ 처리 해야함\n","# 제목, 링크, 날짜, 카테고리, 답변수\n","\n","data=[]\n","pages = int(input('찾는 페이지 수 입력 : '))\n","for i in range(1,pages+1):\n","\n","    url = f'https://kin.naver.com/search/list.naver?query=%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0&page={i}'\n","    response = requests.get(url)\n","\n","    if response.status_code==200:\n","        html = response.text\n","        soup = BeautifulSoup(html,'html.parser')\n","        # print(soup.prettify())\n","        items = soup.select('.basic1>li')\n","        # print(items)\n","\n","\n","        for naverin in items:\n","            title = naverin.select_one('dt').text.strip()\n","            # print(title)\n","            link = naverin.select_one('.basic1>li>dl>dt>a').attrs['href'].strip()\n","            # print(link)\n","            date = naverin.select_one('.txt_inline').text.strip()\n","            # print(date)\n","            categori = naverin.select_one('.txt_g1._nclicks\\\\:kin\\\\.cat2').text.strip()\n","            # print(categori)\n","            answer = naverin.select_one('.hit').text.strip()\n","            # print(answer)\n","        #     # print(title,link,date,categori,answer)\n","            data.append([title,link,date,categori,answer])\n","        df = pd.DataFrame(data,columns = ['제목', '링크', '날짜', '카테고리', '답변수'])\n","        # print(df)\n","        df.to_excel('naverin.xlsx',index=False)\n","        if soup.select_one('.next,_nclicks\\\\:kin\\\\.next')== None: # 페이지가 없으면 멈춤\n","            break\n","\n","    else:\n","        print(\"실패\",response.status_code)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gLbcB2QgadgB"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}